---
chunk_index: 1847
ref: "447deed24dad"
id: "447deed24dad3194439680a108085fd9139653df5761628814578cc2ae2ee8fb"
slug: "full-document--see-and-point-instead-of-remember-and-type"
path: "marker/1987 Apple Human Interface Guidelines - The Apple Desktop Interface/full_document.md"
kind: "markdown"
lines: [428, 449]
token_estimate: 961
content_sha256: "c170d96f7a42b7d98d2d2c9c349f1ff954465d75865c30691a89c4abb295ed93"
compacted: false
heading_path: ["See-and-point (instead of remember-and-type)"]
symbol: null
address: null
asset_path: null
---

# See-and-point (instead of remember-and-type)

Users select actions from alternatives presented on the screen.

The general form of user actions is noun-then-verb, or "Hey, you-do this."

Users rely on recognition, not recall; they shouldn't have to remember anything the computer already knows.

Most programmers have no trouble working with a command-line interface that requires memorization and Boolean logic. The average user is not a programmer.

The Apple Desktop Interface is visually and spatially oriented. The way everything—text, applications, documents, lines, controls—appears on the screen is consistent and well thought out. The screen provides an environment in which people can work effectively, taking full advantage of the power of the computer while enjoying a sensible human environment.

Users interact directly with the screen, choosing objects and activities they are interested in by pointing at them. The mouse is currently the most common pointing device, but other effective pointing devices are available.

There are two fundamental paradigms for how the Apple Desktop Interface works. They share two basic assumptions: that users can see, on the screen, what they're doing; and that they can point at what they see. In one paradigm, users first select an object of interest (the noun) and then select an action (the verb) to be performed on the object. All actions available for the selected object are listed in the menus, so that users who are unsure of what to do next can quickly jog their memory by scanning through them. Users can choose, at any time, any available action, without having to remember any particular command or name. This paradigm requires only recognition, rather than recall, of the desired activities.

In the second paradigm, the user drags an object (the noun) onto some other object which has an action (the verb) associated with it. In the Finder, for example, the user can drag icons into the trash can, into folders, or into disks. No action is chosen from the menus, but it's obvious what happens to the object that is sent to another object. For example, an object sent to the trash can is discarded, and the document sent to a disk icon is copied to that disk. In this variant of the Desktop Interface, users do have to remember what an object such as the trash can is for, so it is especially important that objects look like what they do. If the trash can didn't *look* like the place to discard something, or we didn't know from daily experience that folders contain documents, such an interface wouldn't work. However, when this type of interface is well thought out, it can be easier to learn than menu commands.

Command-line interfaces, on the other hand, require the user to remember a command and type it into the computer. This kind of interface makes considerable demands on the user's memory—especially when the commands are complex or cryptic. Such an interface is especially galling to the new or infrequent user, but it distracts all users from their task and focuses attention instead on the computer's needs.

There are, however, some advantages to the *remember-and-type* approach. Sometimes, when the user is completely certain of what action is desired, a simple keystroke command may be the fastest way to achieve it. For this reason, some desktop applications include *keyboard equivalents* for some menu activities. Keyboard equivalents are a logical extension of the Apple Desktop Interface, fine-tuning it for particular situations. It is essential, however, that keyboard equivalents offer an *alternative* to the see-and-point approach—not a substitute for it. Users who are new to a particular application, or who are looking for potential actions in a confused moment, must always have the option of finding a desired object or action on the screen.